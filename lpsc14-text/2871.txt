 Automatic Image Analysis for Adaptive Instrument Targeting: Applications to MSL and Mars 2020 A. Altinok1,2 and B. Bornstein2 and T. Estlin2 and D. Gaines2 and S. Schaffer2 and D. R. Thompson2 and R. C. Anderson2 and M. Burl2 and R. Castańo2 and R. Wiens3. 1alphan.altinok@jpl.nasa.gov, 2Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Dr., Pasadena CA 91109. 3Los Alamos National Laboratory, Los Alamos, NM. 87545.   Introduction:  Recent planetary rover experiments have demonstrated autonomous science feature detection, enabling immediate response to science targets of opportunity without requiring additional command cycles. Examples on the Mars Exploration Rovers include detecting rocks matching a desired profile [1] and monitoring campaigns for transient dust devils [2].  Here we present new applications onboard image analysis relevant to future rovers, such as an MSL extended mission or in the operations of the proposed Mars 2020 rover.  These missions hold new potential roles for this technology. Point spectroscopy for science targets of opportunity. Instruments such as ChemCam [3] have narrow fields of view and are often targeted at specific features from previous Navigation Camera (NavCam) images.  Operators can only perform these measurements when the rover is stationary or has not driven from the location of the latest downlinked images.  Onboard image analysis allows the rover itself to target instruments based on mid- or end-of-drive imagery.  For example, a bedrock survey could perform a transect in a single command cycle, pausing at intervals to target new outcrops autonomously, rather than having to stop for a full day at each waypoint for operators to select a target. This could provide quality measurements from more diverse locations in a given time period.  It could also enable targeted measurements under desireable midday and end-of-day temperature conditions. Instrument servoing based on image content. Automated image analysis can also be used for fine pointing of standoff instruments.  For example, the ChemCam Remote Microscopic Imager (RMI) can view the microscale surface of the target at distances beyond 1.16m; it reveals features along the ChemCam boresight as well as local context that can be used for fine pointing.  Certain features, such as mineral concretions and veins, are desirable targets but difficult to localize in mesoscale imagery. The ability to automatically analyze standoff images and re-target a spectrometer in a single command cycle could improve its ability to acquire data from these fine-scale features.    Rapid astrobiology survey.   Autonomous targeting of cameras and spectrometers is particularly useful for a caching mission such as the proposed 2020 Mars rover.  The proposal to gather a sample cache for possible return to Earth represents a potentially large investment of resources in the future, so it is desireable from both a risk and cost perspective to survey a large area to find a diverse set of good candidate samples.  Consequently survey efficiency would be a major consideration in mission design.  Improvements in mobility and navigation autonomy have made long singlecommand surveys possible; the recent Mars 2020 Science Definition Team report suggested that Entry Descent and Landing computer hardware might be upgraded in firmware to support faster mobility [4].  Onboard image understanding can complement longer traverses with mid-drive science survey, finding features not visible in orbital remote sensing data and providing geologic context for the few locations where more extensive studies are performed. Methods: We are developing target detection and response capabilities provided by the MER Automated Exploration for Gathering Increased Science (AEGIS) system.  We are augmenting it with MSL-relevant targeting capabilities including a machine learning-based pixel classifier for classifying geologic surfaces [5]. For a future caching mission such as the proposed 2020 rover, it would be desirable to join autonomous science with rapid mobility capabilities.  The "Fast Traverse" system developed at JPL provides FPGAbased stereo image processing for obstacle detection and avoidance, permitting safe navigation at the typical drive speeds of the MSL rover.  We are investigating the integration of both technologies into a single auxiliary processor board that could run them simultaneously during traverse, enabling survey science at speeds demonstrated only in terrestrial settings [6]. We will present simulations of the specific challenge of RMI image analysis. We evaluated target detection capabilities on a selected set of RMI images that had been selected to contain bright vein features as well as dark, localized concretions.  We trained a random forest classifier on three images, and tested the target detection on the remainder.  This simulates an actual mission operations scenario in which operators could train classifiers from historical images and upload them when bandwidth permits.  Random forests have also been shown effective for mesoscale images; we refer the reader to [5] for more details.  Test features were labeled by hand, and the resulting ground truth classifications were used to evaluate the autonomous target selection algorithm.  We thresholded the  classifier output at a high confidence level (95% for veins), and used the centroids of the resulting contiguous regions as target points.   As an alternative for comparison, we placed evenly-spread samples by applying k-means clustering to the pixel locations in the legal (circular) center of the scene. Finding that the definition of concretions was subjective, we opted to score performance on the vein feature. Performance was evaluated based on number of target points needed to hit a vein. Results and discussion: Figure 1 shows several examples of a classification with the original image and automated labeling of the scene.  Here the two targets are vein features (green) and small concretions (not shown). Table 1 shows the quantitative performance of the target selection procedure. Automatic image analysis significantly improves the probability of hitting a target of interest. The precision rate suggests that using autonomous re-targeting instead of a second command cycle would translate into a factor of 4.9 to 14x reduction in the number of command cycles needed.  Such capabilities could also be used to survey a target for a feature before a positive identification; the rover would not spend time resources to conduct the measurement unless the feature was actually present.  Such operations strategies would be relatively risk-free because the rover team could always return to a missed target.  But they would significantly enhance science by enabling operators to deploy the spectrometer more aggressively at mid-drive locations. Acknowledgements: This research was performed at the Jet Propulsion Laboratory, California Institute of Technology.  Copyright 2013 California Institute of Technology.  All rights reserved. U.S. Government Support Acknowledged. The TextureCam project is supported by the NASA Astrobiology Science and Technology Instrument Development program (NNH10ZDA001N-ASTID).  References:  [1]  Estlin, T. A., et al. (2012),  ACM Trans. Intelligent Systems and Technology 3 (3), 50. [2] Castańo, A., et al. (2008) Machine Vision and Applications 19.5-6: 467-482. Maurice, S., et al.. (2012). Space science reviews, 170(1-4), 95-166. [4] Mustard, J. et. al., (2013), Report of the Mars 2020 Science Definition Team, 154 pp., Mars Exploration Program Analysis Group (MEPAG), [5] Wagstaff, K. L., et al. (2013) Geophys. Res. Lett. 40.16: 4188-4193. [6] Thompson., D. R. et al. (2011), Journ. Field Robotics. 28:4, 542-564.  Figure 1: Example classification results.  Left: original image.  Right: automatic feature detection. Vein features are colored green.  Image  (PDS ID) Vein segments  detected False positives Adaptive sampling  Yield (%) Even spacing, Yield (%) CR0_413825167PRC_F0060000CCAM02184M1 64 1 98.5 20.0 CR0_408676341PRC_F0051398CCAM01126M1 21 0 100.0 14.3 CR0_410983834PRC_F0051916CCAM01152M2 27 0 100.0 7.1 CR0_412140460PRC_F0052270CCAM04165M1 51 0 100.0 12.5 Table 1:  Performance comparison of adaptive and evenly-spaced targeting 