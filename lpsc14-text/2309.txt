2309.PDF
DEM Extraction from Stereo Webcam Videos for Small-scale Experimental Geomorphological Modeling.  M.E. Sylvest1, J.C. Dixon1, R.C. Leone2 and A. Barnes3, 1Arkansas Center for Space & Planetary Sciences, University of Arkansas, Fayetteville, AR, 2Colorado School of Mines, Golden, CO, 3Center for Advanced Spatial Technologies, University of Arkansas, Fayetteville, AR.  Introduction: Previously, we performed smallscale experimental simulations of slope evolution under the influence of CO2 sublimation [1]. Recorded with a HD stereo webcam pair, digital elevation models (DEMs) were extracted from the videos. Fully referenced DEMs with a horizontal resolution of 0.34 mm allow quantitative assessment of slope morphological response to simulated environmental conditions [2], (fig.1).    Figure 1. Example DEM of final slope surface with draped color gradient indicating vertical displacement between the first and last DEM surfaces.  The potential usefulness of DEM analysis is limited by the time-consuming DEM creation process, particularly for dynamical applications requiring high physical and temporal resolutions. Even scripted DEM creation remains laborious, requiring significant operator interaction. Fig. 2 summarizes the DEM creation process using Agisoft PhotoScan Professional Edition [3].  The goal of this work is to reduce operator intervention in the DEM creation process, allowing more detailed study of our experimental slope evolution simulations. These same tools are anticipated be applicable to remote sensing imagery, allowing direct comparisons to small-scale laboratory simulations. Although the dimensions of the test section and number of control points is increased, development of the photogrammetric model of the test section followed the procedure described in [1], and are detailed in [2]. The video recording process and DEM analyses are also described in detail in [1] and [2].  Video Synchronization: The Logitech C920 HD webcams used each require a dedicated computer for operation at full 1920 x 1080 resolution. Consequently, each pair of video recordings required synchronization prior to extracting stereo image pairs. Variations in framerates, inherent in the H.264/AVC video coding standard [4], combined with variations due to computer performance, required determination of a mean frame offset between each video pair. This was accomplished by calculating the mean offset of three, visually identified, discrete events in each video pair.  Figure 2. DEM creation workflow summary.  Image Pair Extraction: The QuickTime Pro [5] application programming interface (API) for JavaScript was used to automatically extract image pairs at fixed  time intervals, for each video pair. A small Java application was used to generate the JavaScript, facilitating control parameter input and file system manipulation.  Replacing webcams with video cameras would enable hardware synchronization during recording and eliminate computer-related framerate variability. Removing the requirement for computers  would offset the increased video camera costs, particularly when adding cameras to the array.  Scripting of PhotoScan: All but two steps in the image pair processing workflow have been successfully automated using the PhotoScan Python scripting environment. However, two steps require manual interven2309.pdf
45th Lunar and Planetary Science Conference (2014)
tion: creating image masks, and mapping control points in each image pair. In theory, these tasks could be performed once for each video and the results reused for every image captured from that video. In practice, however, allowance must be made for minute camera movements between each pair capture.   Control Points & Masking: Control points are key to creating high fidelity DEMs. The physical coordinates of these markers establish the coordinate reference frame for the image relative to the cameras. Image masks are used to subtract irrelevant image data outside the region of interest (ROI), i.e. the test section. Both identifying control point coordinates and generating image masks require precise knowledge of the test section boundary coordinates. By combining computer vision and image processing techniques with the known target geometry, we are developing software to automatically locate these boundaries and ultimately, the control points. ImageJ [6] was used interactively to develop the image processing workflow for identifying  control point coordinates as outlined in Fig. [3]. Java Advanced Imaging (JAI) code was integrated with the ImageJ routines to implement the workflow automation.  Figure 3. Control Point Identification Workflow    Figure 4a illustrates an unprocessed webcam image capture. Figure 4b shows the same image after application of an iterative implementation of Li's minimum cross entropy thresholding method [7]. This method was found to consistently identify pixels in the ROI of sample images, while minimizing edge artifacts.   Figure 4. Sample image a) before & b) after thresholding. c) Close-up of two control points after edge detection.  After thresholding, pixels are either black or white. In order to group pixels into regions, the color value of each pixel is examined. If the pixel is white and does not already belong to a region, the pixel is assigned to a new region and unassigned, white, neighboring pixels are also added to that region. After all pixels have been assigned or discarded, regions are iteratively coalesced, based on adjacency, until the number of regions remains constant. While initially assigning pixels to regions, white pixels with one or more black neighboring pixels are identified as candidate ROI boundary pixels. Lines are fitted to the candidate boundary pixels by application of a Hough transformation algorithm [8]. The relative lengths and positions of intersecting linear edges are compared to the known ROI geometry, allowing identification of specific landmarks, particularly the castellated corners at the top of the ROI (fig. 4).  Finally, a Sobel edge detection algorithm [9] is applied to the original image, which outlines the control point markers along the boundary edges (Fig. 4c). The pixel coordinates of the centers of these circular regions are then identified, based on the locations of the previously identified corners and scaling. These are exported for use in the PhotoScan workflow.  Future Work: In addition to replacing webcams with synchronized video cameras, future experiments will be run inside a cryo-vacuum chamber. Although the primary motivation is to more closely simulate Martian conditions, the drastic reduction of humidity will alleviate the glare reflected from water frost on the slope surface.  References: [1] Sylvest, M. et al. (2013) LPS XLIV Abstract, #1626. [2] Leone, R. et al. (2014) LPS XLV Abstract. [3] PhotoScan (Pro. Ed. v.0.9.0, 2012), Agisoft. [4] Schwarz, H. et al. (2007) IEEE Trans. Circuits Syst. Video Technol.17 (9), 1103-1120. [5] QuickTime Pro (v.7.7.4, 2012), Apple Computer. [6] Rasband, W., (GPL 1997). ImageJ (v.1.48), NIH. [7] Landini, G., Auto_Threshold (v.1.15, 2013). [8] Duda, R. and Hart, P. (1971) Comm. ACM 15 (1), 11-15. [9] Rodrigues, L. (2001) Building Imaging Applications with Java Technology, 584.  Acknowledgements: This work was funded in part by the Fulbright College of Arts & Science and the  College of Education & Health Professions, University of Arkansas, Fayetteville, AR. We thank the Center for Advanced Spatial Technologies for photogrammetric services, and C. Owens, Center of Excellence for Poultry Science, for use of laboratory space. 2309.pdf
45th Lunar and Planetary Science Conference (2014)
