1923.PDF
Best Practices in the Evaluation of Public Outreach Events.  W. Cobb1, S. R. Buxner2, S. M. Shebby3, and S.Shipp4 1McREL International, wcobb@mcrel.org; 2 Planetary Science Institute, buxner@psi.edu; 3 McREL International, sshebby@mcrel.org; 1Lunar and Planetary Institute, shipp@lpi.usra.edu.  Introduction:  Over the past decade, the National Aeronautics and Space Administration (NASA) Science Mission Directorate (SMD) has funded four Education and Public Outreach (E/PO) Forums aligned with each of its science divisions: Astrophysics, Earth Science, Heliophysics, and Planetary Science. Together, these Forums help organize individual division E/PO programs into a coordinated, effective, efficient, nationwide effort that shares the scientific discoveries of NASA across a broad array of audiences.  This session will describe the work of the Planetary Science Forum to support community members in one need identified by the Planetary Forum team: public outreach event evaluation. The Forum team sponsored a Best Practices Guide on how to conduct event evaluations. The guide was generated by reviewing published large-scale event evaluation reports; however, the best practices described within are pertinent for all event organizers and evaluators regardless of event size, as following evaluation best practices will allow event organizers to plan events that meet audience needs and systematically demonstrate outputs, outcomes, and impacts related to their events. Public Outreach. Each year, NASA sponsors a variety of public outreach events to share information with educators, students, and the general public. These events are designed to increase interest in and awareness of the mission and goals of NASA. Outreach events range in size from relatively small family science nights at a local school to large-scale mission and celestial event celebrations involving hundreds to thousands of members of the general public.  Methods: McREL International (McREL) staff conducted a literature review of evaluations of largescale public outreach events such as science-related festivals, annual conventions, and exhibitions at public locations. To narrow the process, McREL staff used the following criteria for identifying journal articles and reports: public, science-related events open to the general public attended by at least 1,000 individuals in the past 5 years with associated information about the type of data collected, methodology, and associated results.The articles and evaluation reports were then characterized as either descriptive studies or explanatory studies. Descriptive studies were included if they included information about the planning and implementation of the event.  Explanatory studies were included if they included information about data collection methods, type of data, and the outcomes based on the results. Six published reports were identified for inclusion. For each of these, McREL created an event profile with  information about the event, the methodology used to collect data as part of the evaluation, the findings, such as the outcomes and impacts of the events (disaggregated by respondent group), and other findings of interest related to the successes, challenges, and the lessons learned in the planning and implementation as well as the evaluation of the event. Findings: In general, the data collected were limited to observable demographic information and participant reactions to the event. In most cases, the authors did not specify the analysis techniques used for the qualitative data, though they did provide this information for quantitative data.  The following themes were gleaned from the review.  Use technology to make the job easier. Online surveys were utilized in the majority of events—possibly because online tools can be used to increase the speed of data collection, decrease data entry errors (responses are automatically entered into a database), and are capable of generating graphs and tables.  Design your survey with care. Several event facilitators described difficulties in finding existing items to measure their intended outcomes. However, some also noted difficulties in designing their own surveys due to a lack of experience and expertise. As such, event facilitators should use existing resources to inform survey design and, if possible, gain feedback on draft items through piloting. Collect data at different times and locations. Event facilitators identified several potential issues during data collection, such as the challenge of collecting data in a crowded, informal context  (Jensen and Buckley, 2012) and the importance of timing to ensure visitor feedback reflects desired exposure to the event (Cadenhead & Ong, 2013). As such, event facilitators should be mindful of the timing and location of data collection during the event.  Consider data analysis in advance. At one event, facilitators encountered challenges when using one survey to investigate patterns from different experiences. At another, facilitators found they could not measure changes over time because pre- and post-test data were not linked. Considering the intended use of data before data are collected would help mitigate these challenges.  Be aware of how challenging it is to measure impact. Evaluations must be carefully designed and executed in order to provide evidence of event outcomes and impact. Some event facilitators encounter. Event 1923.pdf
46th Lunar and Planetary Science Conference (2015)
evaluation is often only collected at or after an event. This is not enough to determine whether activities had a lasting effect (impact).  Recommendations:  The Best Practices Guide is offered to help practitioners understand how the evaluation process can be an integral—and valuable—element of event planning and to foster the inclusion of best practices in the implementation of event evaluation. It provides information for planning and conducting event evaluations, outlining how evaluation can be integrated at each stage of event planning and implementation, and offers specific recommendations related to: (1) planning the evaluation, (2) collecting data, (3) analyzing and interpreting data, and (4) sharing evaluation results.  Planning the Evaluation. The “Planning the Evaluation” section of the Guide highlights the need to clarify event goals and develop an aligned evaluation plan so that evaluative information is useful to both event planners and organizers of future events. It includes information evaluation approaches. For example, it describes the need to incorporate evaluation from the project outset, and plan event implementation and evaluation simultaneously. It also provides tips on how to develop evaluation questions aligned to intended outcomes, describes strategies for including key stakeholders in the evaluation process, and identifies several resources that might be helpful when planning event evaluations Collecting Data. The “Collecting Data” section of the Guide provides event providers with strategies on how to identify the preliminary data needed to answer their evaluation questions, as well as how to draft and implement a plan for how the data can be collected. It explains the benefits of collecting quantitative and qualitative data to more fully inform the question at hand and identifies resources for use in drafting instruments, as well as resources to help event facilitators increase response rates and establish clear procedures and processes for data entry, organization, and storage that will support later analysis Analyzing and Interpreting Data. The “Analyzing and Interpreting Data” section of the Guide provides information about how to prepare and then analyze data as well as strategies for data interpretation. Preparing data involves cleaning and verifying data prior to analysis.  This step ensures the data are complete and accurate (for example, duplicate responses should be eliminated).  The quality of data prior to analysis has a direct impact on the quality of subsequent analyses. The process of analysis results in answers to the evaluation questions.  Ideally, decisions about how data will be analyzed, and how it will be grouped and displayed will be written in a data analysis plan prior to the process of data analysis begins. It is natural to form explanations and interpretations about data as they are studied in the context of a particular evaluation question. For example, interpretation of higher attendance rates as compared to last year may be because another popular event was held at the same time or because communication about the event was more strategic than last year.   Sharing Evaluation Results. Once the event is complete and evaluation data synthesized into results, it is important to share the results.  Disseminating findings has several purposes: improving the current event; improving your future events; and enabling other individuals and organizations the opportunity to apply what was learned to their own future events.  These “lessons learned” are presented as recommendations in the written reports reviewed.  This section provides information about reporting the findings of the evaluation, refining and finalizing the evaluation, and sharing the results with other audiences. The Guide also includes detailed “Event profiles" describing the six events referenced in the Findings section and a summary and synthesis of the successes, challenges, and lessons learned related to the planning and implementation of events. Finally, it includes tips and suggestions for situations in which retaining a professional evaluator is not an option.   Primary Sources: Arcand, K., & Watzke, M. (2010). Bringing the universe to the street: A preliminary look at informal learning implications for a largescale non-traditional science outreach project.  Journal of Science Communication, 9(2), 1-13. Frade, A., Fernandes, J., & Doran, R. (2011).  Evaluating the impact of the international Year of Astronomy 2009 in Portugal.  Communicating Astronomy with the Public (CAP) Journal, 11, 35-38. Cadenhead, C. & Ong, A. (2012). Life sciences research weekend 2012 evaluation report. Seattle, WA: Pacific Science Center. Stern, H. (2013). Polar science weekend at Pacific Science Center: Eight years of outreach and partnership. Witness the Artic, 17(2). Retrieved from http://www.arcus.org/witness-the-arctic/2013/2/article/19955 Science Festival Alliance (SFA). (2013). Science Festival Alliance from 2009-2012 – Key findings of independent evaluation. Retrieved from http://sciencefestivals.org/resources/three-years-of-evaluation-intwelves-pages. Jensen, E., & Buckley, N. (2012). Why people attend science festivals: Interests, motivations, and selfreported benefits of public engagement with research. Public Understanding of Science. Retrieved from http://pus.sagepub.com/content/early/2012/10/31/0963662512458624.abstract Additional Information:  This work is supported by NASA’s Science Mission Directorate, Cooperative Agreement Notice NNH09ZDA004C.  1923.pdf
46th Lunar and Planetary Science Conference (2015)
