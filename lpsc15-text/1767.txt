1767.PDF
CALIBRATION TOOLS OF FISH-EYE CAMERAS FOR MONITORING/NAVIGATION IN SPACE MISSIONS. R. Suda1 and H. Demura1, 1Department of Computer Science and Engineering, University of Aizu, Ikki-machi, Aizu-Wakamatsu City, Fukushima 965-8580, Japan (s1170069@gmail.com, demura@u-aizu.ac.jp).   Introduction: Fish-eye cameras are widely used for knowing surrounding environment since they have a ultra-wide field of view (FOV), typically larger than 180˚. Although a general camera has to move its point of view to cover an ultra-wide FOV, a fish-eye camera can cover the same FOV without any movable parts. Thus, fish-eye cameras have advantages in few installation room and payloads in space missions.  The above features of fish-eye cameras are appropriate in some space missions. For example, just few fish-eye cameras can monitor whole inside room in a manned spacecraft; a fish-eye camera mounted on a spacesuit can cover blind zones during an extravehicular activity. In addition, a fish-eye camera can track many objects even if its installed body moves and rotates widely. Thus, fish-eye cameras are appropriate to estimate an attitude of a spacecraft or a lander from images. A satellite’s orbit can be also estimated by the same reason. It is possible to reconstruct 3D scenes from images taken by multiple cameras or a moving camera. The 3D scenes are useful to monitor solid surfaces and to navigate a lander. For instance, Hazcam on Mars Science Laboratory [1] is fish-eye cameras for navigation. Since many miniturized satellites, CubeSat, and piggy-back satellites have strict restrictions than usual ones, using fish-eye cameras is one of the plausible solutions. In contrast to these usages as stated before, fish-eye cameras are tended to be avoided due to their strong distortions: a straight line is projected to a curved line on an image. Geometric distortions can be corrected with application of a fish-eye camera model to images. However, distortions caused by manufacturing errors exist. To estimate attitudes or to reconstruct 3D scenes accurately, these errors have to be corrected by camera calibration. Fish-eye camera calibration, however, is not easy as much as general camera calibration because of different camera models. Therefore, this research shows a fish-eye camera calibration method and develops its software to contribute space missions which use fish-eye cameras.  Fish-eye camera model: A general camera model  assumes the perspective projection model which is expressed by r = f tanθ , where θ  is an angle between an optical axis and an incident ray, r  is a distance from an optical center, and f  is a focal length. Because this cannot represent if θ  is higher than or to 180˚, this is not able to apply to a fish-eye camera. Therefore, this research uses the spherical projection model as shown in Fig. 1 as described in Komagata [2]. In this model, r  is represented as one of 4 projection functions shown in Table 1. These 4 projection functions are decided when fish-eye lens are designed. The projection functions are rewritten with coefficients of radiational distortion such as eq. (1) in case of the equidistant projection. rf0+ a1rf0!"#$%&3+ a2rf0!"#$%&5+ a3rf0!"#$%&7+!= ff0θ,  (1) where ai (i =1,2,3,!)  are distortion coefficients, and f0 is a scale factor to keep powers of r / f0  reasonable range. Shift of an optical center from an image center is represented as (u,v) . In other projections, the left side is same, and the right side is that each projection function is divided by . r  is also represented as eq. (2), and an incident ray  as eq. (3). r = x −u( )2 + y− v( )2  (2) m = x −ur sinθ,y− vr sinθ, cosθ!"#$%&T  (3) Calibration method: The calibration procedures are the followings. I. Take 4 images of LCD which displays 4 different calibration patterns fixing a positional relationship between a fish-eye camera and LCD. II. Change the positional relationship and repeat Step I for several dozen set. III. Detect lines of the patterns from the images. IV. Optimize u,v, f ,a1,a2,a3,!  by constraining colinearity, parallerism, and orthogonarity of the detected lines. f0mTable 1: Projection functions of fish-eye cameras Projection type  Equidistant projection  Stereographic projection  Equi-solid angle projection  Orthographic projection  rfθ2 f tan(θ / 2)2 f sin(θ / 2)f sinθFigure 1: Spherical projection model 1767.pdf
46th Lunar and Planetary Science Conference (2015)
If brightness on a line changes in an image, a line is detected incorrectly. Because reductions of peripheral illumination are strong in fish-eye cameras, this problem influences a lot. To avoid this problem, this research uses a strip pattern and its black/white inversed pattern. Then, crossed points of brightness in both images become lines. Because orthogonal lines are necessary for calibration, a strip pattern, its orthogonal strip pattern, and black/white inversed patterns of each of the 2 strip patterns are taken in Step I.  Points of detected lines are projected on the sphere surface of Fig. 1, and evaluation functions are caluculated for each of colinearity, parallerism, and orthogonarity based on projected points [3]. First and second order differentials of each of the evaluation functions are also calculated. Then, parameters are optimized by Levenberg Marquardt Algorithm [4]. Evaluation: Since truths of parameters are unknown, this research uses simulatited fish-eye images to evaluate how patterns should be taken and how many coefficients are needed. A simulated fish-eye camera are set as that projection is equidistant projection, a focal length is 400 pixels, an image size is 1600x1200 pixels, an optical center is (805, 597), and ai (i =1,2,!, 5)  are 1e-4, 2e-5, 3e-6, 4e-7, and 5e-8. We prepared 3 data sets of pattern images taken by the simulated fish-eye camera: (a) 10 sets, (b) 20 sets, (c) 10 sets Bias. In (a) and (b), we set fihs-eye camera positions so that patterns are projected on images evently. (c) is bias in left side of images. Figure 2 shows each calibration result with relations between θ  and € r . (a) and (b) are almost overlapped on the truth. (c) is, however, different leargely from the truth. Figure 3 shows differences between the truth and calibration results of different numbers of distortion coefficients by using (a). Error is the smallest when the number is 1, and subsequently, 0, 5, 3, 4, and 2.  Discussion and summary: How to take pattern images strongly affects calibration result. Therefore, it is necessary to take pattern images covering its FOV evently. When radiational distortion is not strong, the number of distortion coefficients does not have to be high. And, it should not be 2, 3, or 4 because estimation converges to wrong parameters. However, the number should be over 5 in real environments since the distortion is unknown. Finally, we calibrated a fish-eye camera with 5 radial distortion coefficients. Fig. 4 shows a fish-eye image and its reproject images with calibrated parametes and initial parametes. It shows the calibration done correctly. This fish-eye camera calibration could be used in space missions. The calibration tools can be installed from https://github.com/ryohei-suda/fisheye-library. References: [1] A. Eisenman, et al. (2001) International Symposium on Remote Sensing, 288-297. [2] H. Komagata, et al. (2006) IEICE Trans. on Fundamentals of Electronics, Communications and Computer Sciences, Vol. J89-D, No. 1, 64-73. [3] K. Kanatani (2013) IEEE Trans. Pattern Anal. Mach. Intell., Vol. 35, No. 4, 813-822. [4] L. Kenneth (1944) Quarterly Journal of Applied Mathmatics, Vol. II, No.2, 164-168. Figure 2: Relation between r  andθ  Figure 3: Differences of the numbers of  distortion coefficiences Figure 4: A fish-eye image and  its reprojected images (a) A fish-eye image (b) A reprojected image (c) A reprojected image rotated 60˚ to the left side (d) A reproject image by initial parameters (a) (b) (c) (d) (c) (d) 1767.pdf
46th Lunar and Planetary Science Conference (2015)
