2306.PDF
USE OF IMMERSIVE TECHNOLOGIES IN THE 2015 CANMARS ANALOGUE MISSION.  Z. R. Morse1, G. R. Osinski1, 2, 1Centre for Planetary Science and Exploration, Department of Earth Science, University of Western Ontario, London ON, Canada. zmorse@uwo.ca 2Department of Physics and Astronomy, University of Western Ontario, London ON, Canada.  Introduction: The 2015 CanMars MSR Analogue Mission was a Mars Sample Return Analogue Mission carried out in partnership between the Canadian Space Agency, MacDonald, Dettwiler and Associates Ltd., and the Centre for Planetary Science and Exploration (CPSX) at the University of Western Ontario, as part of the NSERC CREATE project “Technologies and Techniques for Earth and Space Exploration” (create.uwo.ca) [1].) This mission provided an ideal setting to test the use of immersive virtual reality technology to aid scientists in conducting remote field operations.  As the majority of the members of the science team had significantly more experience conducting first-person geologic field activities than remote geologic analysis via a rover, the immersive technologies employed facilitated a more natural and instinctive interpretation of both the terrain and geologic setting of the mission field site. Two experimental immersive technologies were used during the mission: 1) A Google Cardboard virtual reality headset with motion sensitive stereographically projected versions of panoramic images taken by the rover, and 2) an easily navigable dynamically lit 3D terrain model also projected stereographically through a motion sensitive VR platform. Both of these technologies aided in both making science team decisions Background:  The Google Cardboard is a virtual reality (VR) headset that works in concert with a smart phone to create a platform to view data in a visually immersive manner.  The Google Cardboard acts as a frame to house 2 biconvex 45mm lenses and a smart phone.  The screen of the phone projects an image or scene in a side-by-side stereographic format which is focused through the lenses to the user’s eyes allowing the user to focus on the displayed image as if it were in the distance.  The accelerometer within the smart phone is used to track the phone’s orientation and update the image or scene displayed as the user turns his or her head, creating the illusion of looking around a scene while standing at a fixed point within it.  Similar devices have been used by NASA to simulate an immersive environment around active rovers on Mars and assist in navigational and scientific decision making [2]. Methods:  The immersive panorama were created by taking the panoramic images taken by the Mars Exploration Science Rover (MESR) and reprojecting them in a cylindrical format with the left and right extents of the image brought together.  This can be done manually in an image editing program, or automatically using and app or online program.  For ease of both creation and sharing the immersive images between team members and devices, the online service RoundMe was used to create the cylindrical projections.  Once rendered the cylindrical projections could be viewed by anyone with a computer, smartphone or tablet by accessing a URL for the specific image.  When accessed by a smartphone, the RoundMe online service has an integrated option to automatically project the scene in stereo for use with a VR headset.  With this option selected an observer simply placed their phone within the Google Cardboard and was able to view the scene depicted in a panorama taken by MESR from the rover’s perspective. The immersive terrain model was generated through several steps.  First the Digital Terrain Model provided prior to the start of the mission was rendered in the ENVI GIS program.  This DEM has a vertical resolution of 10m and covers most of the projected landing ellipse for the mission.  Once rendered in 3D at maximum resolution, this DEM was exported and converted in to a Virtual Reality Modeling Language (VRML) format.  The VRML file was then uploaded to an online virtual model-rendering program called Sketchfab.  Once in Sketchfab the blank terrain of the VRML file was overlain with a 60cm per pixel color QuickBird image of the landing site. As SketchFab is not a GIS program and the in-sim team was not provided with any georeferenced images, the image overlay process had to be manually georeferenced to the topographic features of the 3D terrain model.  Once aligned properly the team had an easily navigable high-resolution model of the landing site (Fig 1). This 3D model could be shared by team members via a URL and rendered on any computer, smartphone, or tablet, eliminating the need for specific GIS software to view a 3D model of the landing area.  The 3D model could also be viewed in a stereographic projection through Sketchfab (Fig 2).  When viewed on a smartphone and combined with the Google Cardboard device, the 3D model could also be viewed immersively. A user controls the viewing angle of the model by turning his or her head while the accelerometer within the smartphone tracks the look direction and updates the seen accordingly.  This feature allowed team scientists to view the terrain surrounding the rover as if they were standing at a fixed point within the scene looking in any direction. 2306.pdf
47th Lunar and Planetary Science Conference (2016)
          Figure 1. 3D terrain model rendered in the Sketchfab program with dynamic lighting.           Figure 2. Stereographically projected Sketchfab 3D model of landing site terrain.  Google Cardboard Device in foreground.  The image projected through the VR device is shown here on screen. Results:  The cylindrically projected panoramic images viewed through the Google Cardboard VR headset functioned exceptionally well.  When viewing the near 360º panoramic images projected as a flat image, several team members reported difficulty discerning the direction or location of features.  In panoramas taken a low sun angles it took team members several seconds to minutes to resolve the location of the sun with the direction of the shadows cast by features in the flat panoramic projection.  When the same images were viewed immersively via the Google Cardboard as cylindrical projections, the delays in perception were eliminated.  Team members using the immersive panoramas immediately recognized the location of the rover and surrounding features. The immersive panorama also provided an inherent sense of scale as the body of the rover could be observed in the foreground of the image, rather than being split between the far most right and left extents of the flat panoramic images. For more on the panoramic images see [3]  The 3D terrain model did not work as well from an immersive standpoint. When viewed through the Google Cardboard platform the vertical resolution of the initial DEM was not high enough to match the quality of the panoramic images.  Users focused on the distortions present based on the underlying DEM and noted that the sensitivity of the phone accelerometer was too high.  This did not enable a user to smoothly look about the scene and would often cause the model to jump off screen.   The 3D model was effective when used in a standard viewing format.  The ability to access the 3D terrain model via a URL without the use of a dedicated GIS program proved to be helpful for sharing the model between team members. The Sketchfab program allowed for easy navigation throughout the model and for the viewing perspective to be altered smoothly so that the scene could be observed from multiple heights and angles.  Sketchfab also provides a set of lighting tools enabling the team to predict what rock outcrops would be visible or shaded at certain times of day.  The Symphony program used to command MESR [4] also provides this feature, but with Symphony only installed on some computers and requiring some training to utilize properly, the Sketchfab lighting model functioned as a quick backup for estimating lighting angles.   Conclusions:  Both of these immersive technologies aided the science team in interpreting the rover’s surroundings.  The ability to both share the visualizations across platforms and to view the data in a more natural and immersive manner was certainly helpful during the mission.  The immersive panoramas and 3D terrain model also proved to be extremely beneficial as tools for illustrating the rover operations to the public through outreach events and media outlets.  The ability to engage the public by allowing individuals to place themselves virtually in the place of the rover was key to facilitating an instant understanding of the complexity, importance, and excitement of this analogue mission [5]. Both of these immersive technologies proved to be good supplements to traditional data view methods.  They have not yet been developed to a level at which they could replace more traditional methods of interacting with images and topographic data from a rover mission, but their inclusion in this mission was an helpful way to gain a better perspective of rover operations from existing data products. References: [1] [1] Osinski G. R., et al. (2016) LPSC XLVII, (this conference). [2] NASA Onsight; http://www.jpl.nasa.gov/news/news.php?feature=4451. [3] Harrison T., et al. (2016) LPSC XLVII, (this conference). [4] Sapers, H. M., et al. (2016) LPSC XLVII, (this conference). [5] Hill P. J. A., et al. (2016) LPSC XLVII, (this conference). Acknowledgements: This work was funded by the Natural Sciences and Engineering Research Council of Canada’s CREATE program and the Canadian Space Agency.  We thank the teams at Google Cardboard, Roundme, and Sketchfab for the use of their products and assistance. 2306.pdf
47th Lunar and Planetary Science Conference (2016)
